[[34m2022-11-18 06:11:11,068[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for jobs_analysis to 2022-11-17T00:00:00+00:00, run_after=2022-11-18T00:00:00+00:00[0m
[[34m2022-11-18 06:11:11,453[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for jobs_analysis to 2022-11-18T00:00:00+00:00, run_after=2022-11-19T00:00:00+00:00[0m
[[34m2022-11-18 06:11:11,496[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 2 tasks up for execution:
	<TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Process_Data_Lake manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:11:11,497[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:11:11,497[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 1/16 running and queued tasks[0m
[[34m2022-11-18 06:11:11,497[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Process_Data_Lake manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:11:11,498[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Process_Data_Lake', run_id='scheduled__2022-11-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2022-11-18 06:11:11,498[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:11:11,499[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Process_Data_Lake', run_id='manual__2022-11-18T06:11:10.201511+00:00', try_number=1, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2022-11-18 06:11:11,499[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:11:11,503[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:11:11,990[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:11:12,767[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-16T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:12:21,290[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:12:21,783[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:12:22,556[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Process_Data_Lake manual__2022-11-18T06:11:10.201511+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:13:25,435[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Process_Data_Lake run_id=scheduled__2022-11-16T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:13:25,436[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Process_Data_Lake run_id=manual__2022-11-18T06:11:10.201511+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:13:25,441[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Process_Data_Lake, run_id=manual__2022-11-18T06:11:10.201511+00:00, map_index=-1, run_start_date=2022-11-18 06:12:22.577784+00:00, run_end_date=2022-11-18 06:13:25.126015+00:00, run_duration=62.548231, state=success, executor_state=success, try_number=1, max_tries=3, job_id=3, pool=default_pool, queue=default, priority_weight=11, operator=BashOperator, queued_dttm=2022-11-18 06:11:11.497520+00:00, queued_by_job_id=1, pid=5801[0m
[[34m2022-11-18 06:13:25,442[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Process_Data_Lake, run_id=scheduled__2022-11-16T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:11:12.789710+00:00, run_end_date=2022-11-18 06:12:20.975649+00:00, run_duration=68.185939, state=success, executor_state=success, try_number=1, max_tries=3, job_id=2, pool=default_pool, queue=default, priority_weight=11, operator=BashOperator, queued_dttm=2022-11-18 06:11:11.497520+00:00, queued_by_job_id=1, pid=5362[0m
[[34m2022-11-18 06:13:25,452[0m] {[34mmanager.py:[0m301} ERROR[0m - DagFileProcessorManager (PID=3781) last sent a heartbeat 134.01 seconds ago! Restarting it[0m
[[34m2022-11-18 06:13:25,455[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 3781. PIDs of all processes in the group: [3781][0m
[[34m2022-11-18 06:13:25,455[0m] {[34mprocess_utils.py:[0m84} INFO[0m - Sending the signal Signals.SIGTERM to group 3781[0m
[[34m2022-11-18 06:13:25,587[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=3781, status='terminated', exitcode=0, started='06:02:39') (3781) terminated with exit code 0[0m
[[34m2022-11-18 06:13:25,591[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 6239[0m
[[34m2022-11-18 06:13:25,595[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-11-18T06:13:25.606+0000] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-11-18 06:13:25,617[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:13:25,619[0m] {[34mscheduler_job.py:[0m1404} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-11-18 06:13:25,792[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 3 tasks up for execution:
	<TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:13:25,792[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:13:25,792[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 1/16 running and queued tasks[0m
[[34m2022-11-18 06:13:25,792[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 2/16 running and queued tasks[0m
[[34m2022-11-18 06:13:25,792[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:13:25,794[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Process_Data_Lake', run_id='scheduled__2022-11-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2022-11-18 06:13:25,794[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:13:25,794[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='scheduled__2022-11-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:13:25,794[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:13:25,794[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='manual__2022-11-18T06:11:10.201511+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:13:25,794[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:13:25,798[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:13:26,288[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:13:27,069[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-17T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:14:30,686[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:14:31,180[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:14:31,961[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:14:32,431[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:14:32,922[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:14:33,705[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:14:34,128[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Process_Data_Lake run_id=scheduled__2022-11-17T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:14:34,128[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=scheduled__2022-11-16T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:14:34,128[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=manual__2022-11-18T06:11:10.201511+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:14:34,135[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=manual__2022-11-18T06:11:10.201511+00:00, map_index=-1, run_start_date=2022-11-18 06:14:33.725913+00:00, run_end_date=2022-11-18 06:14:33.820706+00:00, run_duration=0.094793, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=6, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:13:25.793075+00:00, queued_by_job_id=1, pid=6683[0m
[[34m2022-11-18 06:14:34,135[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=scheduled__2022-11-16T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:14:31.982777+00:00, run_end_date=2022-11-18 06:14:32.083500+00:00, run_duration=0.100723, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=5, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:13:25.793075+00:00, queued_by_job_id=1, pid=6681[0m
[[34m2022-11-18 06:14:34,135[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Process_Data_Lake, run_id=scheduled__2022-11-17T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:13:27.090997+00:00, run_end_date=2022-11-18 06:14:30.344953+00:00, run_duration=63.253956, state=success, executor_state=success, try_number=1, max_tries=3, job_id=4, pool=default_pool, queue=default, priority_weight=11, operator=BashOperator, queued_dttm=2022-11-18 06:13:25.793075+00:00, queued_by_job_id=1, pid=6242[0m
[[34m2022-11-18 06:14:34,145[0m] {[34mmanager.py:[0m301} ERROR[0m - DagFileProcessorManager (PID=6239) last sent a heartbeat 68.39 seconds ago! Restarting it[0m
[[34m2022-11-18 06:14:34,148[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 6239. PIDs of all processes in the group: [6239][0m
[[34m2022-11-18 06:14:34,148[0m] {[34mprocess_utils.py:[0m84} INFO[0m - Sending the signal Signals.SIGTERM to group 6239[0m
[[34m2022-11-18 06:14:34,280[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=6239, status='terminated', exitcode=0, started='06:13:25') (6239) terminated with exit code 0[0m
[[34m2022-11-18 06:14:34,284[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 6684[0m
[[34m2022-11-18 06:14:34,289[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-11-18T06:14:34.300+0000] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-11-18 06:14:34,485[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:14:34,486[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:14:34,486[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:14:34,487[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='scheduled__2022-11-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:14:34,487[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:14:34,491[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:14:34,985[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:14:35,766[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:14:36,224[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=scheduled__2022-11-17T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:14:36,230[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=scheduled__2022-11-17T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:14:35.789509+00:00, run_end_date=2022-11-18 06:14:35.889424+00:00, run_duration=0.099915, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=7, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:14:34.486349+00:00, queued_by_job_id=1, pid=6687[0m
[[34m2022-11-18 06:18:25,779[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:19:33,032[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:19:33,032[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:19:33,032[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:19:33,033[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='scheduled__2022-11-16T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:19:33,033[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:19:33,038[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:19:33,536[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:19:34,314[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:19:34,729[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=scheduled__2022-11-16T00:00:00+00:00 exited with status success for try_number 2[0m
[[34m2022-11-18 06:19:34,732[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=scheduled__2022-11-16T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:19:34.336566+00:00, run_end_date=2022-11-18 06:19:34.431545+00:00, run_duration=0.094979, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=8, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:19:33.032827+00:00, queued_by_job_id=1, pid=7004[0m
[[34m2022-11-18 06:19:34,913[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:19:34,913[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:19:34,913[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:19:34,914[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='manual__2022-11-18T06:11:10.201511+00:00', try_number=2, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:19:34,914[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:19:34,919[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:19:35,410[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:19:36,186[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:19:36,605[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=manual__2022-11-18T06:11:10.201511+00:00 exited with status success for try_number 2[0m
[[34m2022-11-18 06:19:36,608[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=manual__2022-11-18T06:11:10.201511+00:00, map_index=-1, run_start_date=2022-11-18 06:19:36.207719+00:00, run_end_date=2022-11-18 06:19:36.300871+00:00, run_duration=0.093152, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=9, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:19:34.913704+00:00, queued_by_job_id=1, pid=7007[0m
[[34m2022-11-18 06:19:36,768[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:19:36,769[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:19:36,769[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:19:36,770[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='scheduled__2022-11-17T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:19:36,770[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:19:36,773[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:19:37,274[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:19:38,063[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:19:38,491[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=scheduled__2022-11-17T00:00:00+00:00 exited with status success for try_number 2[0m
[[34m2022-11-18 06:19:38,494[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=scheduled__2022-11-17T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:19:38.084295+00:00, run_end_date=2022-11-18 06:19:38.179801+00:00, run_duration=0.095506, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=10, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:19:36.769487+00:00, queued_by_job_id=1, pid=7010[0m
[[34m2022-11-18 06:23:25,937[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:24:35,079[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:24:35,080[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:24:35,080[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:24:35,081[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='scheduled__2022-11-16T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:24:35,081[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:24:35,085[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:24:35,573[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:24:36,342[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:24:36,756[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=scheduled__2022-11-16T00:00:00+00:00 exited with status success for try_number 3[0m
[[34m2022-11-18 06:24:36,759[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=scheduled__2022-11-16T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:24:36.371571+00:00, run_end_date=2022-11-18 06:24:36.465046+00:00, run_duration=0.093475, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:24:35.080444+00:00, queued_by_job_id=1, pid=7325[0m
[[34m2022-11-18 06:24:36,916[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:24:36,916[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:24:36,916[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:24:36,917[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='manual__2022-11-18T06:11:10.201511+00:00', try_number=3, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:24:36,917[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:24:36,921[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:24:37,413[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:24:38,185[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:24:38,606[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=manual__2022-11-18T06:11:10.201511+00:00 exited with status success for try_number 3[0m
[[34m2022-11-18 06:24:38,609[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=manual__2022-11-18T06:11:10.201511+00:00, map_index=-1, run_start_date=2022-11-18 06:24:38.207089+00:00, run_end_date=2022-11-18 06:24:38.299292+00:00, run_duration=0.092203, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=12, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:24:36.916801+00:00, queued_by_job_id=1, pid=7328[0m
[[34m2022-11-18 06:24:38,767[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:24:38,768[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:24:38,768[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:24:38,769[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='scheduled__2022-11-17T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:24:38,769[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:24:38,773[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:24:39,260[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:24:40,037[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:24:40,444[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=scheduled__2022-11-17T00:00:00+00:00 exited with status success for try_number 3[0m
[[34m2022-11-18 06:24:40,447[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=scheduled__2022-11-17T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:24:40.059780+00:00, run_end_date=2022-11-18 06:24:40.154762+00:00, run_duration=0.094982, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=13, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:24:38.768448+00:00, queued_by_job_id=1, pid=7331[0m
[[34m2022-11-18 06:28:26,095[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:29:36,581[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:29:36,581[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:29:36,581[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:29:36,582[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='scheduled__2022-11-16T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:29:36,583[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:29:36,586[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:29:37,077[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:29:37,864[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-16T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:29:38,352[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=scheduled__2022-11-16T00:00:00+00:00 exited with status success for try_number 4[0m
[[34m2022-11-18 06:29:38,356[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=scheduled__2022-11-16T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:29:37.887098+00:00, run_end_date=2022-11-18 06:29:37.983660+00:00, run_duration=0.096562, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=14, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:29:36.582145+00:00, queued_by_job_id=1, pid=7655[0m
[[34m2022-11-18 06:29:38,517[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:29:38,517[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:29:38,517[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [scheduled]>[0m
[[34m2022-11-18 06:29:38,518[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='manual__2022-11-18T06:11:10.201511+00:00', try_number=4, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:29:38,518[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:29:38,522[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:11:10.201511+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:29:39,020[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:29:39,821[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:11:10.201511+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:29:40,283[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=manual__2022-11-18T06:11:10.201511+00:00 exited with status success for try_number 4[0m
[[34m2022-11-18 06:29:40,286[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=manual__2022-11-18T06:11:10.201511+00:00, map_index=-1, run_start_date=2022-11-18 06:29:39.843723+00:00, run_end_date=2022-11-18 06:29:39.937715+00:00, run_duration=0.093992, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=15, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:29:38.517925+00:00, queued_by_job_id=1, pid=7658[0m
[[34m2022-11-18 06:29:40,454[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:29:40,454[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:29:40,454[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [scheduled]>[0m
[[34m2022-11-18 06:29:40,455[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='scheduled__2022-11-17T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:29:40,455[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:29:40,459[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'scheduled__2022-11-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:29:40,947[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:29:41,722[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables scheduled__2022-11-17T00:00:00+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:29:42,183[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=scheduled__2022-11-17T00:00:00+00:00 exited with status success for try_number 4[0m
[[34m2022-11-18 06:29:42,186[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=scheduled__2022-11-17T00:00:00+00:00, map_index=-1, run_start_date=2022-11-18 06:29:41.744190+00:00, run_end_date=2022-11-18 06:29:41.839097+00:00, run_duration=0.094907, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=16, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:29:40.454635+00:00, queued_by_job_id=1, pid=7661[0m
[[34m2022-11-18 06:29:42,325[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun jobs_analysis @ 2022-11-16 00:00:00+00:00: scheduled__2022-11-16T00:00:00+00:00, state:running, queued_at: 2022-11-18 06:11:11.060814+00:00. externally triggered: False> failed[0m
[[34m2022-11-18 06:29:42,325[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=jobs_analysis, execution_date=2022-11-16 00:00:00+00:00, run_id=scheduled__2022-11-16T00:00:00+00:00, run_start_date=2022-11-18 06:11:11.078497+00:00, run_end_date=2022-11-18 06:29:42.325932+00:00, run_duration=1111.247435, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-11-16 00:00:00+00:00, data_interval_end=2022-11-17 00:00:00+00:00, dag_hash=cbfff67dd3f678f0f60cc0ccbd48dc90[0m
[[34m2022-11-18 06:29:42,330[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for jobs_analysis to 2022-11-17T00:00:00+00:00, run_after=2022-11-18T00:00:00+00:00[0m
[[34m2022-11-18 06:29:43,470[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for jobs_analysis to 2022-11-18T00:00:00+00:00, run_after=2022-11-19T00:00:00+00:00[0m
[[34m2022-11-18 06:29:43,488[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun jobs_analysis @ 2022-11-18 06:11:10.201511+00:00: manual__2022-11-18T06:11:10.201511+00:00, state:running, queued_at: 2022-11-18 06:11:10.216935+00:00. externally triggered: True> failed[0m
[[34m2022-11-18 06:29:43,488[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=jobs_analysis, execution_date=2022-11-18 06:11:10.201511+00:00, run_id=manual__2022-11-18T06:11:10.201511+00:00, run_start_date=2022-11-18 06:11:11.078630+00:00, run_end_date=2022-11-18 06:29:43.488499+00:00, run_duration=1112.409869, state=failed, external_trigger=True, run_type=manual, data_interval_start=2022-11-17 00:00:00+00:00, data_interval_end=2022-11-18 00:00:00+00:00, dag_hash=cbfff67dd3f678f0f60cc0ccbd48dc90[0m
[[34m2022-11-18 06:29:43,491[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for jobs_analysis to 2022-11-18T00:00:00+00:00, run_after=2022-11-19T00:00:00+00:00[0m
[[34m2022-11-18 06:29:44,639[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun jobs_analysis @ 2022-11-17 00:00:00+00:00: scheduled__2022-11-17T00:00:00+00:00, state:running, queued_at: 2022-11-18 06:11:11.450280+00:00. externally triggered: False> failed[0m
[[34m2022-11-18 06:29:44,639[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=jobs_analysis, execution_date=2022-11-17 00:00:00+00:00, run_id=scheduled__2022-11-17T00:00:00+00:00, run_start_date=2022-11-18 06:11:11.462604+00:00, run_end_date=2022-11-18 06:29:44.639356+00:00, run_duration=1113.176752, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-11-17 00:00:00+00:00, data_interval_end=2022-11-18 00:00:00+00:00, dag_hash=cbfff67dd3f678f0f60cc0ccbd48dc90[0m
[[34m2022-11-18 06:29:44,641[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for jobs_analysis to 2022-11-18T00:00:00+00:00, run_after=2022-11-19T00:00:00+00:00[0m
[[34m2022-11-18 06:31:39,634[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Process_Data_Lake manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>[0m
[[34m2022-11-18 06:31:39,634[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:31:39,634[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Process_Data_Lake manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>[0m
[[34m2022-11-18 06:31:39,635[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Process_Data_Lake', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2022-11-18 06:31:39,635[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:31:39,640[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:31:40,135[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:31:40,913[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Process_Data_Lake manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:32:42,891[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Process_Data_Lake run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:32:42,895[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Process_Data_Lake, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:31:40.935091+00:00, run_end_date=2022-11-18 06:32:42.556039+00:00, run_duration=61.620948, state=success, executor_state=success, try_number=1, max_tries=3, job_id=17, pool=default_pool, queue=default, priority_weight=11, operator=BashOperator, queued_dttm=2022-11-18 06:31:39.634852+00:00, queued_by_job_id=1, pid=7787[0m
[[34m2022-11-18 06:32:42,906[0m] {[34mmanager.py:[0m301} ERROR[0m - DagFileProcessorManager (PID=6684) last sent a heartbeat 63.30 seconds ago! Restarting it[0m
[[34m2022-11-18 06:32:42,908[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 6684. PIDs of all processes in the group: [6684][0m
[[34m2022-11-18 06:32:42,908[0m] {[34mprocess_utils.py:[0m84} INFO[0m - Sending the signal Signals.SIGTERM to group 6684[0m
[[34m2022-11-18 06:32:43,040[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=6684, status='terminated', exitcode=0, started='06:14:33') (6684) terminated with exit code 0[0m
[[34m2022-11-18 06:32:43,044[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 8225[0m
[[34m2022-11-18 06:32:43,049[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-11-18T06:32:43.060+0000] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-11-18 06:32:43,229[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>[0m
[[34m2022-11-18 06:32:43,229[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:32:43,230[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>[0m
[[34m2022-11-18 06:32:43,231[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Create_Tables', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2022-11-18 06:32:43,231[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:43,234[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Create_Tables', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:43,728[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:32:44,513[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Create_Tables manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:32:45,081[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Create_Tables run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:32:45,084[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Create_Tables, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:32:44.534815+00:00, run_end_date=2022-11-18 06:32:44.718490+00:00, run_duration=0.183675, state=success, executor_state=success, try_number=1, max_tries=3, job_id=18, pool=default_pool, queue=default, priority_weight=10, operator=PostgresOperator, queued_dttm=2022-11-18 06:32:43.230293+00:00, queued_by_job_id=1, pid=8228[0m
[[34m2022-11-18 06:32:45,229[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 6 tasks up for execution:
	<TaskInstance: jobs_analysis.Copy_Jobs_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_Company_location_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_Job_Ratings_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_Job_Sector_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_Job_Salaries_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_employees_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>[0m
[[34m2022-11-18 06:32:45,229[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:32:45,229[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 1/16 running and queued tasks[0m
[[34m2022-11-18 06:32:45,229[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 2/16 running and queued tasks[0m
[[34m2022-11-18 06:32:45,229[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 3/16 running and queued tasks[0m
[[34m2022-11-18 06:32:45,229[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 4/16 running and queued tasks[0m
[[34m2022-11-18 06:32:45,229[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 5/16 running and queued tasks[0m
[[34m2022-11-18 06:32:45,229[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Copy_Jobs_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_Company_location_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_Job_Ratings_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_Job_Sector_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_Job_Salaries_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Copy_employees_Details manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>[0m
[[34m2022-11-18 06:32:45,230[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Copy_Jobs_Details', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2022-11-18 06:32:45,231[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Jobs_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:45,231[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Copy_Company_location_Details', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2022-11-18 06:32:45,231[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Company_location_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:45,231[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Copy_Job_Ratings_Details', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2022-11-18 06:32:45,231[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Job_Ratings_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:45,231[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Copy_Job_Sector_Details', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2022-11-18 06:32:45,231[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Job_Sector_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:45,231[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Copy_Job_Salaries_Details', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2022-11-18 06:32:45,231[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Job_Salaries_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:45,231[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Copy_employees_Details', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2022-11-18 06:32:45,231[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_employees_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:45,235[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Jobs_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:32:45,730[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:32:46,517[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Copy_Jobs_Details manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:33:03,957[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Company_location_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:04,451[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:33:05,236[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Copy_Company_location_Details manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:33:07,105[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Job_Ratings_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:07,595[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:33:08,375[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Copy_Job_Ratings_Details manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:33:10,005[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Job_Sector_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:10,496[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:33:11,279[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Copy_Job_Sector_Details manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:33:13,388[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_Job_Salaries_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:13,883[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:33:14,666[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Copy_Job_Salaries_Details manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:33:16,241[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Copy_employees_Details', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:16,742[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:33:17,539[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Copy_employees_Details manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:33:20,200[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Copy_Jobs_Details run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:33:20,200[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Copy_Company_location_Details run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:33:20,200[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Copy_Job_Ratings_Details run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:33:20,200[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Copy_Job_Sector_Details run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:33:20,201[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Copy_Job_Salaries_Details run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:33:20,201[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Copy_employees_Details run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:33:20,204[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Copy_Company_location_Details, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:33:05.258376+00:00, run_end_date=2022-11-18 06:33:06.774865+00:00, run_duration=1.516489, state=success, executor_state=success, try_number=1, max_tries=3, job_id=20, pool=default_pool, queue=default, priority_weight=4, operator=CopyToRedshiftOperator, queued_dttm=2022-11-18 06:32:45.230058+00:00, queued_by_job_id=1, pid=8233[0m
[[34m2022-11-18 06:33:20,205[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Copy_Job_Ratings_Details, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:33:08.396518+00:00, run_end_date=2022-11-18 06:33:09.695107+00:00, run_duration=1.298589, state=success, executor_state=success, try_number=1, max_tries=3, job_id=21, pool=default_pool, queue=default, priority_weight=4, operator=CopyToRedshiftOperator, queued_dttm=2022-11-18 06:32:45.230058+00:00, queued_by_job_id=1, pid=8235[0m
[[34m2022-11-18 06:33:20,205[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Copy_Job_Salaries_Details, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:33:14.687236+00:00, run_end_date=2022-11-18 06:33:15.870277+00:00, run_duration=1.183041, state=success, executor_state=success, try_number=1, max_tries=3, job_id=23, pool=default_pool, queue=default, priority_weight=4, operator=CopyToRedshiftOperator, queued_dttm=2022-11-18 06:32:45.230058+00:00, queued_by_job_id=1, pid=8239[0m
[[34m2022-11-18 06:33:20,205[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Copy_Job_Sector_Details, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:33:11.302910+00:00, run_end_date=2022-11-18 06:33:13.082552+00:00, run_duration=1.779642, state=success, executor_state=success, try_number=1, max_tries=3, job_id=22, pool=default_pool, queue=default, priority_weight=4, operator=CopyToRedshiftOperator, queued_dttm=2022-11-18 06:32:45.230058+00:00, queued_by_job_id=1, pid=8237[0m
[[34m2022-11-18 06:33:20,205[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Copy_Jobs_Details, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:32:46.538796+00:00, run_end_date=2022-11-18 06:33:03.625765+00:00, run_duration=17.086969, state=success, executor_state=success, try_number=1, max_tries=3, job_id=19, pool=default_pool, queue=default, priority_weight=4, operator=CopyToRedshiftOperator, queued_dttm=2022-11-18 06:32:45.230058+00:00, queued_by_job_id=1, pid=8231[0m
[[34m2022-11-18 06:33:20,205[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Copy_employees_Details, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:33:17.561821+00:00, run_end_date=2022-11-18 06:33:19.862000+00:00, run_duration=2.300179, state=success, executor_state=success, try_number=1, max_tries=3, job_id=24, pool=default_pool, queue=default, priority_weight=4, operator=CopyToRedshiftOperator, queued_dttm=2022-11-18 06:32:45.230058+00:00, queued_by_job_id=1, pid=8241[0m
[[34m2022-11-18 06:33:20,366[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 2 tasks up for execution:
	<TaskInstance: jobs_analysis.Count_and_Null_Test manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Table_Relation_Test manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>[0m
[[34m2022-11-18 06:33:20,366[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 0/16 running and queued tasks[0m
[[34m2022-11-18 06:33:20,366[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG jobs_analysis has 1/16 running and queued tasks[0m
[[34m2022-11-18 06:33:20,366[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: jobs_analysis.Count_and_Null_Test manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>
	<TaskInstance: jobs_analysis.Table_Relation_Test manual__2022-11-18T06:31:37.875994+00:00 [scheduled]>[0m
[[34m2022-11-18 06:33:20,367[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Count_and_Null_Test', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-11-18 06:33:20,367[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Count_and_Null_Test', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:20,367[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='jobs_analysis', task_id='Table_Relation_Test', run_id='manual__2022-11-18T06:31:37.875994+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-11-18 06:33:20,368[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Table_Relation_Test', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:20,371[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Count_and_Null_Test', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:20,874[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:33:21,677[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Count_and_Null_Test manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:33:22,269[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Table_Relation_Test', 'manual__2022-11-18T06:31:37.875994+00:00', '--local', '--subdir', 'DAGS_FOLDER/jobs_dag.py'][0m
[[34m2022-11-18 06:33:22,764[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/ubuntu/theinsightco_project/data_pipeline/dags/jobs_dag.py[0m
[[34m2022-11-18 06:33:23,552[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: jobs_analysis.Table_Relation_Test manual__2022-11-18T06:31:37.875994+00:00 [queued]> on host ip-172-31-2-28.us-west-1.compute.internal[0m
[[34m2022-11-18 06:33:24,227[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Count_and_Null_Test run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:33:24,227[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of jobs_analysis.Table_Relation_Test run_id=manual__2022-11-18T06:31:37.875994+00:00 exited with status success for try_number 1[0m
[[34m2022-11-18 06:33:24,231[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Count_and_Null_Test, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:33:21.700316+00:00, run_end_date=2022-11-18 06:33:21.960768+00:00, run_duration=0.260452, state=success, executor_state=success, try_number=1, max_tries=3, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=DataQualityOperator, queued_dttm=2022-11-18 06:33:20.366888+00:00, queued_by_job_id=1, pid=8244[0m
[[34m2022-11-18 06:33:24,231[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=jobs_analysis, task_id=Table_Relation_Test, run_id=manual__2022-11-18T06:31:37.875994+00:00, map_index=-1, run_start_date=2022-11-18 06:33:23.574155+00:00, run_end_date=2022-11-18 06:33:23.884399+00:00, run_duration=0.310244, state=success, executor_state=success, try_number=1, max_tries=3, job_id=26, pool=default_pool, queue=default, priority_weight=2, operator=DataQualityOperator, queued_dttm=2022-11-18 06:33:20.366888+00:00, queued_by_job_id=1, pid=8246[0m
[[34m2022-11-18 06:33:24,365[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun jobs_analysis @ 2022-11-18 06:31:37.875994+00:00: manual__2022-11-18T06:31:37.875994+00:00, state:running, queued_at: 2022-11-18 06:31:37.884267+00:00. externally triggered: True> successful[0m
[[34m2022-11-18 06:33:24,365[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=jobs_analysis, execution_date=2022-11-18 06:31:37.875994+00:00, run_id=manual__2022-11-18T06:31:37.875994+00:00, run_start_date=2022-11-18 06:31:38.457754+00:00, run_end_date=2022-11-18 06:33:24.365741+00:00, run_duration=105.907987, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-11-17 00:00:00+00:00, data_interval_end=2022-11-18 00:00:00+00:00, dag_hash=cbfff67dd3f678f0f60cc0ccbd48dc90[0m
[[34m2022-11-18 06:33:24,368[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for jobs_analysis to 2022-11-18T00:00:00+00:00, run_after=2022-11-19T00:00:00+00:00[0m
[[34m2022-11-18 06:33:26,232[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:38:26,370[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:43:26,508[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:48:26,648[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:53:26,786[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 06:58:26,923[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:03:27,060[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:08:27,207[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:13:27,340[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:18:27,477[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:23:27,614[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:28:27,752[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:33:27,888[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:38:27,994[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:43:28,093[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:48:28,200[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:53:28,337[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 07:58:28,473[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:03:28,610[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:08:28,747[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:13:28,884[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:18:29,022[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:23:29,116[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:28:29,254[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:33:29,391[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:38:29,528[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:43:29,552[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:48:29,690[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:53:29,828[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-18 08:58:29,965[0m] {[34mscheduler_job.py:[0m1381} INFO[0m - Resetting orphaned tasks for active dag runs[0m
