[2022-11-18T06:13:27.090+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-17T00:00:00+00:00 [queued]>
[2022-11-18T06:13:27.095+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-17T00:00:00+00:00 [queued]>
[2022-11-18T06:13:27.095+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-11-18T06:13:27.095+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 4
[2022-11-18T06:13:27.095+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-11-18T06:13:27.106+0000] {taskinstance.py:1383} INFO - Executing <Task(BashOperator): Process_Data_Lake> on 2022-11-17 00:00:00+00:00
[2022-11-18T06:13:27.109+0000] {standard_task_runner.py:55} INFO - Started process 6242 to run task
[2022-11-18T06:13:27.111+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'jobs_analysis', 'Process_Data_Lake', 'scheduled__2022-11-17T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/jobs_dag.py', '--cfg-path', '/tmp/tmp5hhvicb2']
[2022-11-18T06:13:27.113+0000] {standard_task_runner.py:83} INFO - Job 4: Subtask Process_Data_Lake
[2022-11-18T06:13:27.146+0000] {task_command.py:376} INFO - Running <TaskInstance: jobs_analysis.Process_Data_Lake scheduled__2022-11-17T00:00:00+00:00 [running]> on host ip-172-31-2-28.us-west-1.compute.internal
[2022-11-18T06:13:27.181+0000] {taskinstance.py:1590} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=lohitha.vanteru@sjsu.edu
AIRFLOW_CTX_DAG_OWNER=Theinsightco
AIRFLOW_CTX_DAG_ID=jobs_analysis
AIRFLOW_CTX_TASK_ID=Process_Data_Lake
AIRFLOW_CTX_EXECUTION_DATE=2022-11-17T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-11-17T00:00:00+00:00
[2022-11-18T06:13:27.182+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2022-11-18T06:13:27.182+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'process-data-from-lake']
[2022-11-18T06:13:27.188+0000] {subprocess.py:86} INFO - Output:
[2022-11-18T06:13:29.038+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/home/ubuntu/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2022-11-18T06:13:29.090+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /home/ubuntu/.ivy2/cache
[2022-11-18T06:13:29.091+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /home/ubuntu/.ivy2/jars
[2022-11-18T06:13:29.095+0000] {subprocess.py:93} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2022-11-18T06:13:29.097+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-35397860-da24-4ee5-82a0-3ed97ee03004;1.0
[2022-11-18T06:13:29.097+0000] {subprocess.py:93} INFO - 	confs: [default]
[2022-11-18T06:13:29.224+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-aws;3.3.4 in central
[2022-11-18T06:13:29.252+0000] {subprocess.py:93} INFO - 	found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2022-11-18T06:13:29.276+0000] {subprocess.py:93} INFO - 	found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2022-11-18T06:13:29.300+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 195ms :: artifacts dl 9ms
[2022-11-18T06:13:29.300+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2022-11-18T06:13:29.300+0000] {subprocess.py:93} INFO - 	com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2022-11-18T06:13:29.300+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]
[2022-11-18T06:13:29.301+0000] {subprocess.py:93} INFO - 	org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2022-11-18T06:13:29.301+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2022-11-18T06:13:29.301+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2022-11-18T06:13:29.301+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2022-11-18T06:13:29.301+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2022-11-18T06:13:29.301+0000] {subprocess.py:93} INFO - 	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
[2022-11-18T06:13:29.301+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2022-11-18T06:13:29.305+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-35397860-da24-4ee5-82a0-3ed97ee03004
[2022-11-18T06:13:29.305+0000] {subprocess.py:93} INFO - 	confs: [default]
[2022-11-18T06:13:29.311+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 3 already retrieved (0kB/6ms)
[2022-11-18T06:13:29.516+0000] {subprocess.py:93} INFO - 22/11/18 06:13:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-11-18T06:13:29.686+0000] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2022-11-18T06:13:29.687+0000] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2022-11-18T06:13:34.877+0000] {subprocess.py:93} INFO - 22/11/18 06:13:34 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2022-11-18T06:14:29.538+0000] {subprocess.py:93} INFO - [Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 14:>                                                         (0 + 1) / 1]                                                                                [Stage 15:>                                                         (0 + 1) / 1]                                                                                [Stage 18:>                                                         (0 + 1) / 1]                                                                                [Stage 21:>                                                         (0 + 1) / 1]                                                                                [Stage 24:>                                                         (0 + 1) / 1]                                                                                [Stage 28:>                                                         (0 + 1) / 1]                                                                                [Stage 30:>                                                         (0 + 1) / 1]                                                                                [Stage 31:>                                                         (0 + 1) / 1]                                                                                [Stage 34:>                                                         (0 + 1) / 1]                                                                                [Stage 38:>                                                         (0 + 1) / 1]                                                                                [Stage 39:>                                                         (0 + 1) / 1]                                                                                [Stage 42:>                                                         (0 + 1) / 1]                                                                                [Stage 46:>                                                         (0 + 1) / 1]                                                                                [Stage 47:>                                                         (0 + 1) / 1]                                                                                [Stage 50:>                                                         (0 + 1) / 1]                                                                                /home/ubuntu/theinsightco_project/data_lake/lake.cfg
[2022-11-18T06:14:30.318+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2022-11-18T06:14:30.345+0000] {taskinstance.py:1401} INFO - Marking task as SUCCESS. dag_id=jobs_analysis, task_id=Process_Data_Lake, execution_date=20221117T000000, start_date=20221118T061327, end_date=20221118T061430
[2022-11-18T06:14:30.374+0000] {local_task_job.py:164} INFO - Task exited with return code 0
[2022-11-18T06:14:30.394+0000] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
